{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24adc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "XGBoost version: 3.0.5\n",
      "Optuna version: 4.5.0\n",
      "LightGBM version: 4.6.0\n",
      "Parsed XGBoost major version: 3\n",
      "Script started at: 2025-10-10 21:56:53\n",
      "Train shape: (517754, 14)\n",
      "Test shape: (172585, 13)\n",
      "Sample shape: (172585, 2)\n",
      "Train columns: ['id', 'road_type', 'num_lanes', 'curvature', 'speed_limit', 'lighting', 'weather', 'road_signs_present', 'public_road', 'time_of_day', 'holiday', 'school_season', 'num_reported_accidents', 'accident_risk']\n",
      "Column data types:\n",
      "id                          int64\n",
      "road_type                  object\n",
      "num_lanes                   int64\n",
      "curvature                 float64\n",
      "speed_limit                 int64\n",
      "lighting                   object\n",
      "weather                    object\n",
      "road_signs_present           bool\n",
      "public_road                  bool\n",
      "time_of_day                object\n",
      "holiday                      bool\n",
      "school_season                bool\n",
      "num_reported_accidents      int64\n",
      "accident_risk             float64\n",
      "dtype: object\n",
      "Missing values in train:\n",
      "id                        0\n",
      "road_type                 0\n",
      "num_lanes                 0\n",
      "curvature                 0\n",
      "speed_limit               0\n",
      "lighting                  0\n",
      "weather                   0\n",
      "road_signs_present        0\n",
      "public_road               0\n",
      "time_of_day               0\n",
      "holiday                   0\n",
      "school_season             0\n",
      "num_reported_accidents    0\n",
      "accident_risk             0\n",
      "dtype: int64\n",
      "Missing values in test:\n",
      "id                        0\n",
      "road_type                 0\n",
      "num_lanes                 0\n",
      "curvature                 0\n",
      "speed_limit               0\n",
      "lighting                  0\n",
      "weather                   0\n",
      "road_signs_present        0\n",
      "public_road               0\n",
      "time_of_day               0\n",
      "holiday                   0\n",
      "school_season             0\n",
      "num_reported_accidents    0\n",
      "dtype: int64\n",
      "Target skewness: 0.378\n",
      "Target min: 0.000, max: 1.000, mean: 0.352\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero risk rows: 608\n",
      "Negative risk rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 21:56:58,178] A new study created in memory with name: no-name-5295dee8-38a6-4ca5-9830-ab018b65bd70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n",
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 21:57:21,105] Trial 0 finished with value: 0.056368996260518604 and parameters: {'learning_rate': 0.00883751632848639, 'n_estimators': 1084, 'num_leaves': 42, 'max_depth': 6, 'min_data_in_leaf': 11, 'subsample': 0.8409823020811233, 'colsample_bytree': 0.8318377371768648}. Best is trial 0 with value: 0.056368996260518604.\n",
      "[I 2025-10-10 21:59:48,304] Trial 1 finished with value: 0.05631849987529044 and parameters: {'learning_rate': 0.0065840234525563835, 'n_estimators': 1974, 'num_leaves': 41, 'max_depth': 10, 'min_data_in_leaf': 29, 'subsample': 0.7432814445672417, 'colsample_bytree': 0.8474492965972255}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:01:12,871] Trial 2 finished with value: 0.05636427647838075 and parameters: {'learning_rate': 0.010527583597782546, 'n_estimators': 901, 'num_leaves': 40, 'max_depth': 10, 'min_data_in_leaf': 20, 'subsample': 0.8876636699466954, 'colsample_bytree': 0.7371617503195317}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:02:01,705] Trial 3 finished with value: 0.05640095940462016 and parameters: {'learning_rate': 0.009769251018219727, 'n_estimators': 1120, 'num_leaves': 31, 'max_depth': 8, 'min_data_in_leaf': 34, 'subsample': 0.8160216163095919, 'colsample_bytree': 0.7139434273211973}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:02:16,442] Trial 4 finished with value: 0.05640021211354964 and parameters: {'learning_rate': 0.013755915220250322, 'n_estimators': 1070, 'num_leaves': 33, 'max_depth': 5, 'min_data_in_leaf': 13, 'subsample': 0.7756194694705636, 'colsample_bytree': 0.7636212455072772}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:02:27,071] Trial 5 finished with value: 0.05655145950898224 and parameters: {'learning_rate': 0.013012009000530421, 'n_estimators': 612, 'num_leaves': 24, 'max_depth': 9, 'min_data_in_leaf': 29, 'subsample': 0.7564162030380497, 'colsample_bytree': 0.7158553051681785}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:02:43,270] Trial 6 finished with value: 0.056381195872122675 and parameters: {'learning_rate': 0.01003760696786427, 'n_estimators': 916, 'num_leaves': 39, 'max_depth': 8, 'min_data_in_leaf': 21, 'subsample': 0.8225501943689488, 'colsample_bytree': 0.7950188396752762}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:02:59,190] Trial 7 finished with value: 0.056369955980342494 and parameters: {'learning_rate': 0.014085683179885563, 'n_estimators': 1036, 'num_leaves': 28, 'max_depth': 10, 'min_data_in_leaf': 20, 'subsample': 0.7948657692526605, 'colsample_bytree': 0.7268598692125318}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:03:16,994] Trial 8 finished with value: 0.056396931842792215 and parameters: {'learning_rate': 0.011547420567827685, 'n_estimators': 1327, 'num_leaves': 24, 'max_depth': 10, 'min_data_in_leaf': 30, 'subsample': 0.803296913308959, 'colsample_bytree': 0.7711345510135065}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:03:32,661] Trial 9 finished with value: 0.05657017623316864 and parameters: {'learning_rate': 0.007535806126462449, 'n_estimators': 994, 'num_leaves': 46, 'max_depth': 5, 'min_data_in_leaf': 24, 'subsample': 0.8973786356399978, 'colsample_bytree': 0.8222660009558557}. Best is trial 1 with value: 0.05631849987529044.\n",
      "[I 2025-10-10 22:04:02,751] A new study created in memory with name: no-name-7cfea4f9-1afb-4f9b-a0ed-45a478f7433a\n",
      "[I 2025-10-10 22:04:02,759] Trial 0 finished with value: inf and parameters: {'learning_rate': 0.006914543886440783, 'n_estimators': 1350, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.8375305686100493, 'colsample_bytree': 0.8288042850810691}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,764] Trial 1 finished with value: inf and parameters: {'learning_rate': 0.006306425010128976, 'n_estimators': 1109, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8808883599147754, 'colsample_bytree': 0.8867983813334368}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,768] Trial 2 finished with value: inf and parameters: {'learning_rate': 0.00784296902622761, 'n_estimators': 1095, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.8945035917286519, 'colsample_bytree': 0.9228021505959598}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,772] Trial 3 finished with value: inf and parameters: {'learning_rate': 0.006042189652496899, 'n_estimators': 533, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.8511454496751134, 'colsample_bytree': 0.9049185512551376}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,775] Trial 4 finished with value: inf and parameters: {'learning_rate': 0.014314322040162095, 'n_estimators': 798, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.9055226211423311, 'colsample_bytree': 0.8483780177558685}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,779] Trial 5 finished with value: inf and parameters: {'learning_rate': 0.007558881199747898, 'n_estimators': 517, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.8016514169944374, 'colsample_bytree': 0.9278519688111417}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,782] Trial 6 finished with value: inf and parameters: {'learning_rate': 0.007169041299191226, 'n_estimators': 822, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.8486137914250689, 'colsample_bytree': 0.8078500486213368}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,786] Trial 7 finished with value: inf and parameters: {'learning_rate': 0.005010545232559425, 'n_estimators': 1224, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.9101154698377928, 'colsample_bytree': 0.8743628801132816}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,790] Trial 8 finished with value: inf and parameters: {'learning_rate': 0.009505412566927697, 'n_estimators': 535, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.8022441541367329, 'colsample_bytree': 0.9204395145358737}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:04:02,794] Trial 9 finished with value: inf and parameters: {'learning_rate': 0.008335839472053614, 'n_estimators': 540, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.8417143953329668, 'colsample_bytree': 0.9281786877569227}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "Warning: XGBoost early stopping not supported. Training without early stopping.\n",
      "Fold 1 RMSE (ensemble): 0.05626 with weights (0.6, 0.4)\n",
      "Fold 1 RMSE (LightGBM only): 0.05632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:04:41,735] A new study created in memory with name: no-name-f9fccf84-3248-46d0-8215-fb30df7b400c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:05:04,868] Trial 0 finished with value: 0.056219892859307646 and parameters: {'learning_rate': 0.01167759781374526, 'n_estimators': 1982, 'num_leaves': 21, 'max_depth': 6, 'min_data_in_leaf': 31, 'subsample': 0.7242126390888538, 'colsample_bytree': 0.8590335655211023}. Best is trial 0 with value: 0.056219892859307646.\n",
      "[I 2025-10-10 22:05:30,299] Trial 1 finished with value: 0.05618356380638281 and parameters: {'learning_rate': 0.007215981937892834, 'n_estimators': 1350, 'num_leaves': 46, 'max_depth': 7, 'min_data_in_leaf': 27, 'subsample': 0.7237721782409693, 'colsample_bytree': 0.7240233106430768}. Best is trial 1 with value: 0.05618356380638281.\n",
      "[I 2025-10-10 22:05:39,443] Trial 2 finished with value: 0.05652818671212758 and parameters: {'learning_rate': 0.009131003018365957, 'n_estimators': 572, 'num_leaves': 28, 'max_depth': 7, 'min_data_in_leaf': 26, 'subsample': 0.7311221397388863, 'colsample_bytree': 0.7934884323402175}. Best is trial 1 with value: 0.05618356380638281.\n",
      "[I 2025-10-10 22:05:50,224] Trial 3 finished with value: 0.05724036986817034 and parameters: {'learning_rate': 0.006175410790286849, 'n_estimators': 552, 'num_leaves': 44, 'max_depth': 7, 'min_data_in_leaf': 12, 'subsample': 0.7464062651013269, 'colsample_bytree': 0.7360798073599906}. Best is trial 1 with value: 0.05618356380638281.\n",
      "[I 2025-10-10 22:06:01,383] Trial 4 finished with value: 0.056476109655924624 and parameters: {'learning_rate': 0.008656206403380587, 'n_estimators': 841, 'num_leaves': 23, 'max_depth': 5, 'min_data_in_leaf': 27, 'subsample': 0.7710908875773386, 'colsample_bytree': 0.7964689375100277}. Best is trial 1 with value: 0.05618356380638281.\n",
      "[I 2025-10-10 22:06:32,365] Trial 5 finished with value: 0.05614183033645185 and parameters: {'learning_rate': 0.007063988708891244, 'n_estimators': 1791, 'num_leaves': 45, 'max_depth': 10, 'min_data_in_leaf': 11, 'subsample': 0.7240817942263502, 'colsample_bytree': 0.8612125028066568}. Best is trial 5 with value: 0.05614183033645185.\n",
      "[I 2025-10-10 22:06:52,915] Trial 6 finished with value: 0.05617012650423463 and parameters: {'learning_rate': 0.009843299724586906, 'n_estimators': 1200, 'num_leaves': 44, 'max_depth': 7, 'min_data_in_leaf': 17, 'subsample': 0.8609003020179109, 'colsample_bytree': 0.7978378066752363}. Best is trial 5 with value: 0.05614183033645185.\n",
      "[I 2025-10-10 22:07:10,901] Trial 7 finished with value: 0.056199004063468234 and parameters: {'learning_rate': 0.0118610966230151, 'n_estimators': 1270, 'num_leaves': 31, 'max_depth': 6, 'min_data_in_leaf': 27, 'subsample': 0.7243223011389502, 'colsample_bytree': 0.8860584380742812}. Best is trial 5 with value: 0.05614183033645185.\n",
      "[I 2025-10-10 22:07:18,876] Trial 8 finished with value: 0.056493809497427994 and parameters: {'learning_rate': 0.011374586492934362, 'n_estimators': 524, 'num_leaves': 45, 'max_depth': 5, 'min_data_in_leaf': 23, 'subsample': 0.83420600922766, 'colsample_bytree': 0.7069006210767507}. Best is trial 5 with value: 0.05614183033645185.\n",
      "[I 2025-10-10 22:07:40,762] Trial 9 finished with value: 0.05612085997061233 and parameters: {'learning_rate': 0.017223299423072637, 'n_estimators': 1453, 'num_leaves': 40, 'max_depth': 6, 'min_data_in_leaf': 37, 'subsample': 0.7308514226042522, 'colsample_bytree': 0.7787443737856384}. Best is trial 9 with value: 0.05612085997061233.\n",
      "[I 2025-10-10 22:07:58,795] A new study created in memory with name: no-name-e6e8f7e3-88bf-4c86-b19b-bdb9ca4398b7\n",
      "[I 2025-10-10 22:07:58,801] Trial 0 finished with value: inf and parameters: {'learning_rate': 0.005787493311923285, 'n_estimators': 972, 'max_depth': 8, 'min_child_weight': 5, 'subsample': 0.8206262954170662, 'colsample_bytree': 0.946474794534468}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,804] Trial 1 finished with value: inf and parameters: {'learning_rate': 0.017920835212995607, 'n_estimators': 1348, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.8196639421321066, 'colsample_bytree': 0.8021797458897455}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,807] Trial 2 finished with value: inf and parameters: {'learning_rate': 0.0071295784672744935, 'n_estimators': 1396, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.8146760790891356, 'colsample_bytree': 0.8441517433869727}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,811] Trial 3 finished with value: inf and parameters: {'learning_rate': 0.008094710920413969, 'n_estimators': 881, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8291446426493742, 'colsample_bytree': 0.8570043292384133}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,815] Trial 4 finished with value: inf and parameters: {'learning_rate': 0.008880238362324627, 'n_estimators': 1211, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.8160060059026727, 'colsample_bytree': 0.8692669415128877}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,818] Trial 5 finished with value: inf and parameters: {'learning_rate': 0.009427977204423272, 'n_estimators': 1281, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.8289882776069103, 'colsample_bytree': 0.9074449400710024}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,821] Trial 6 finished with value: inf and parameters: {'learning_rate': 0.010292942251542031, 'n_estimators': 902, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8587734324548135, 'colsample_bytree': 0.822045555217645}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,825] Trial 7 finished with value: inf and parameters: {'learning_rate': 0.006133726626884937, 'n_estimators': 526, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.9105584965323974, 'colsample_bytree': 0.9488269933616523}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,828] Trial 8 finished with value: inf and parameters: {'learning_rate': 0.006526620052733261, 'n_estimators': 1496, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.918596075032956, 'colsample_bytree': 0.8105582889442772}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:07:58,831] Trial 9 finished with value: inf and parameters: {'learning_rate': 0.00557597440421029, 'n_estimators': 602, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.8620597058982175, 'colsample_bytree': 0.858028070878869}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "Warning: XGBoost early stopping not supported. Training without early stopping.\n",
      "Fold 2 RMSE (ensemble): 0.05606 with weights (0.6, 0.4)\n",
      "Fold 2 RMSE (LightGBM only): 0.05612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:08:30,202] A new study created in memory with name: no-name-e82b1e0a-ab97-4990-bcef-e368f445a0f8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:08:40,474] Trial 0 finished with value: 0.05630531444520649 and parameters: {'learning_rate': 0.017524345813766686, 'n_estimators': 696, 'num_leaves': 34, 'max_depth': 5, 'min_data_in_leaf': 17, 'subsample': 0.8951462760500735, 'colsample_bytree': 0.7775155461169273}. Best is trial 0 with value: 0.05630531444520649.\n",
      "[I 2025-10-10 22:08:51,508] Trial 1 finished with value: 0.056255351192132555 and parameters: {'learning_rate': 0.014071866998627672, 'n_estimators': 598, 'num_leaves': 42, 'max_depth': 7, 'min_data_in_leaf': 31, 'subsample': 0.7904572542041952, 'colsample_bytree': 0.701858939869127}. Best is trial 1 with value: 0.056255351192132555.\n",
      "[I 2025-10-10 22:09:05,348] Trial 2 finished with value: 0.0563775518553117 and parameters: {'learning_rate': 0.007873903619009718, 'n_estimators': 708, 'num_leaves': 41, 'max_depth': 8, 'min_data_in_leaf': 17, 'subsample': 0.8289681770961905, 'colsample_bytree': 0.797219694395588}. Best is trial 1 with value: 0.056255351192132555.\n",
      "[I 2025-10-10 22:09:27,757] Trial 3 finished with value: 0.056138912892438976 and parameters: {'learning_rate': 0.01869622648046047, 'n_estimators': 1835, 'num_leaves': 47, 'max_depth': 7, 'min_data_in_leaf': 17, 'subsample': 0.8508949951098448, 'colsample_bytree': 0.7761171823402948}. Best is trial 3 with value: 0.056138912892438976.\n",
      "[I 2025-10-10 22:09:39,518] Trial 4 finished with value: 0.056838549678048925 and parameters: {'learning_rate': 0.0073225013162051735, 'n_estimators': 554, 'num_leaves': 48, 'max_depth': 10, 'min_data_in_leaf': 35, 'subsample': 0.7100309543335411, 'colsample_bytree': 0.7100714682613695}. Best is trial 3 with value: 0.056138912892438976.\n",
      "[I 2025-10-10 22:10:00,040] Trial 5 finished with value: 0.056221471313437354 and parameters: {'learning_rate': 0.011609902827773178, 'n_estimators': 1626, 'num_leaves': 23, 'max_depth': 10, 'min_data_in_leaf': 25, 'subsample': 0.8158738509664526, 'colsample_bytree': 0.7430280432886477}. Best is trial 3 with value: 0.056138912892438976.\n",
      "[I 2025-10-10 22:10:15,635] Trial 6 finished with value: 0.05643121489033954 and parameters: {'learning_rate': 0.008002235986670206, 'n_estimators': 1161, 'num_leaves': 20, 'max_depth': 7, 'min_data_in_leaf': 31, 'subsample': 0.7887043767849992, 'colsample_bytree': 0.8550967597290485}. Best is trial 3 with value: 0.056138912892438976.\n",
      "[I 2025-10-10 22:10:31,754] Trial 7 finished with value: 0.056142975405775584 and parameters: {'learning_rate': 0.019049230682356585, 'n_estimators': 1061, 'num_leaves': 48, 'max_depth': 10, 'min_data_in_leaf': 25, 'subsample': 0.7531901325491447, 'colsample_bytree': 0.7615253227170212}. Best is trial 3 with value: 0.056138912892438976.\n",
      "[I 2025-10-10 22:10:43,785] Trial 8 finished with value: 0.05631814060195645 and parameters: {'learning_rate': 0.014849754242850163, 'n_estimators': 905, 'num_leaves': 20, 'max_depth': 9, 'min_data_in_leaf': 10, 'subsample': 0.8470747434383814, 'colsample_bytree': 0.732854373164147}. Best is trial 3 with value: 0.056138912892438976.\n",
      "[I 2025-10-10 22:11:11,776] Trial 9 finished with value: 0.05616179470210003 and parameters: {'learning_rate': 0.014920592600133675, 'n_estimators': 1985, 'num_leaves': 40, 'max_depth': 6, 'min_data_in_leaf': 13, 'subsample': 0.7785422338510862, 'colsample_bytree': 0.8688378734307776}. Best is trial 3 with value: 0.056138912892438976.\n",
      "[I 2025-10-10 22:11:32,316] A new study created in memory with name: no-name-5eb60ec0-0daa-4892-afa3-bc026dbc318d\n",
      "[I 2025-10-10 22:11:32,322] Trial 0 finished with value: inf and parameters: {'learning_rate': 0.013325881876383321, 'n_estimators': 900, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.875074369869089, 'colsample_bytree': 0.8246269628614881}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,325] Trial 1 finished with value: inf and parameters: {'learning_rate': 0.006093297037538955, 'n_estimators': 659, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8784416135977838, 'colsample_bytree': 0.8724791913160529}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,328] Trial 2 finished with value: inf and parameters: {'learning_rate': 0.009668518660914789, 'n_estimators': 845, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.8328236647926939, 'colsample_bytree': 0.8852425928030871}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,331] Trial 3 finished with value: inf and parameters: {'learning_rate': 0.007083122602487184, 'n_estimators': 1417, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.911946320997684, 'colsample_bytree': 0.8830835893278377}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,334] Trial 4 finished with value: inf and parameters: {'learning_rate': 0.005477359137464354, 'n_estimators': 832, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.9420090787935727, 'colsample_bytree': 0.8949690878332138}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,338] Trial 5 finished with value: inf and parameters: {'learning_rate': 0.006362323862836314, 'n_estimators': 748, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9118795040350703, 'colsample_bytree': 0.8827966106131081}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,341] Trial 6 finished with value: inf and parameters: {'learning_rate': 0.012275465609401657, 'n_estimators': 1448, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.8997799312539349, 'colsample_bytree': 0.8958844517551807}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,345] Trial 7 finished with value: inf and parameters: {'learning_rate': 0.019797411859223964, 'n_estimators': 1277, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.9372393359300495, 'colsample_bytree': 0.8991903806374514}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,349] Trial 8 finished with value: inf and parameters: {'learning_rate': 0.01951269313029684, 'n_estimators': 690, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8024408471456728, 'colsample_bytree': 0.9432082340516067}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:11:32,353] Trial 9 finished with value: inf and parameters: {'learning_rate': 0.01859079529661382, 'n_estimators': 753, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.9332573250852938, 'colsample_bytree': 0.8275606692699211}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "Warning: XGBoost early stopping not supported. Training without early stopping.\n",
      "Fold 3 RMSE (ensemble): 0.05614 with weights (0.9, 0.1)\n",
      "Fold 3 RMSE (LightGBM only): 0.05614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:11:54,084] A new study created in memory with name: no-name-18e2517f-a8f4-4fce-a837-e792ff325aae\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:12:05,685] Trial 0 finished with value: 0.056028339516393515 and parameters: {'learning_rate': 0.018699072347611395, 'n_estimators': 723, 'num_leaves': 43, 'max_depth': 8, 'min_data_in_leaf': 28, 'subsample': 0.7250850186368023, 'colsample_bytree': 0.881237611765916}. Best is trial 0 with value: 0.056028339516393515.\n",
      "[I 2025-10-10 22:12:33,853] Trial 1 finished with value: 0.05608657038955398 and parameters: {'learning_rate': 0.005500744152831989, 'n_estimators': 1616, 'num_leaves': 45, 'max_depth': 7, 'min_data_in_leaf': 12, 'subsample': 0.7849482982027316, 'colsample_bytree': 0.8883778181368431}. Best is trial 0 with value: 0.056028339516393515.\n",
      "[I 2025-10-10 22:12:55,722] Trial 2 finished with value: 0.05616370936184733 and parameters: {'learning_rate': 0.00832034371567777, 'n_estimators': 1649, 'num_leaves': 25, 'max_depth': 5, 'min_data_in_leaf': 38, 'subsample': 0.7568950059779815, 'colsample_bytree': 0.738636723313577}. Best is trial 0 with value: 0.056028339516393515.\n",
      "[I 2025-10-10 22:13:20,423] Trial 3 finished with value: 0.05610664874715522 and parameters: {'learning_rate': 0.006600806774306531, 'n_estimators': 1555, 'num_leaves': 35, 'max_depth': 7, 'min_data_in_leaf': 18, 'subsample': 0.7114952362496245, 'colsample_bytree': 0.8462641672697127}. Best is trial 0 with value: 0.056028339516393515.\n",
      "[I 2025-10-10 22:13:36,610] Trial 4 finished with value: 0.056024136766926635 and parameters: {'learning_rate': 0.014164261030114072, 'n_estimators': 1002, 'num_leaves': 44, 'max_depth': 8, 'min_data_in_leaf': 19, 'subsample': 0.8804380041794563, 'colsample_bytree': 0.8144548488459077}. Best is trial 4 with value: 0.056024136766926635.\n",
      "[I 2025-10-10 22:13:48,690] Trial 5 finished with value: 0.05618516450275374 and parameters: {'learning_rate': 0.011186673836017305, 'n_estimators': 920, 'num_leaves': 33, 'max_depth': 5, 'min_data_in_leaf': 32, 'subsample': 0.7620242913461401, 'colsample_bytree': 0.7338117172469277}. Best is trial 4 with value: 0.056024136766926635.\n",
      "[I 2025-10-10 22:14:16,881] Trial 6 finished with value: 0.05603094658000344 and parameters: {'learning_rate': 0.006562185204149026, 'n_estimators': 1765, 'num_leaves': 50, 'max_depth': 7, 'min_data_in_leaf': 33, 'subsample': 0.8670257748152875, 'colsample_bytree': 0.8532594717909068}. Best is trial 4 with value: 0.056024136766926635.\n",
      "[I 2025-10-10 22:14:40,842] Trial 7 finished with value: 0.05607534581151769 and parameters: {'learning_rate': 0.006239204188684797, 'n_estimators': 1555, 'num_leaves': 41, 'max_depth': 8, 'min_data_in_leaf': 37, 'subsample': 0.7767977443175232, 'colsample_bytree': 0.8068198645954466}. Best is trial 4 with value: 0.056024136766926635.\n",
      "[I 2025-10-10 22:14:55,220] Trial 8 finished with value: 0.05608677075478376 and parameters: {'learning_rate': 0.01834077475004388, 'n_estimators': 1472, 'num_leaves': 20, 'max_depth': 6, 'min_data_in_leaf': 22, 'subsample': 0.7599559706746253, 'colsample_bytree': 0.7390576878140981}. Best is trial 4 with value: 0.056024136766926635.\n",
      "[I 2025-10-10 22:15:10,668] Trial 9 finished with value: 0.05619116497913143 and parameters: {'learning_rate': 0.009612961812964095, 'n_estimators': 1181, 'num_leaves': 44, 'max_depth': 5, 'min_data_in_leaf': 22, 'subsample': 0.8987916645026406, 'colsample_bytree': 0.8982263230691008}. Best is trial 4 with value: 0.056024136766926635.\n",
      "[I 2025-10-10 22:15:24,085] A new study created in memory with name: no-name-feba0baf-3f9b-46b6-bf96-fb846c3d92d0\n",
      "[I 2025-10-10 22:15:24,092] Trial 0 finished with value: inf and parameters: {'learning_rate': 0.018427765050505966, 'n_estimators': 1134, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.9384802403098931, 'colsample_bytree': 0.9017095544860981}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,095] Trial 1 finished with value: inf and parameters: {'learning_rate': 0.018678158612966386, 'n_estimators': 632, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8177124383943597, 'colsample_bytree': 0.8999070020094982}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,098] Trial 2 finished with value: inf and parameters: {'learning_rate': 0.0129435483916444, 'n_estimators': 1110, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.8836866247557094, 'colsample_bytree': 0.8614340111973826}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,101] Trial 3 finished with value: inf and parameters: {'learning_rate': 0.013685765537730134, 'n_estimators': 1026, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.855438877712085, 'colsample_bytree': 0.878470444034362}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,105] Trial 4 finished with value: inf and parameters: {'learning_rate': 0.007254315285186411, 'n_estimators': 1199, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.8893238802673711, 'colsample_bytree': 0.8518428785467974}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,108] Trial 5 finished with value: inf and parameters: {'learning_rate': 0.013329821695841945, 'n_estimators': 1226, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.8731315649633663, 'colsample_bytree': 0.9156152724058503}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,112] Trial 6 finished with value: inf and parameters: {'learning_rate': 0.01661988877504327, 'n_estimators': 1250, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.9103741308712691, 'colsample_bytree': 0.8892495159992878}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,116] Trial 7 finished with value: inf and parameters: {'learning_rate': 0.018100498363031837, 'n_estimators': 1039, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.8975832153622244, 'colsample_bytree': 0.8699291184603969}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,120] Trial 8 finished with value: inf and parameters: {'learning_rate': 0.011577423820365695, 'n_estimators': 1454, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.879098963146419, 'colsample_bytree': 0.8021672603408775}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:15:24,125] Trial 9 finished with value: inf and parameters: {'learning_rate': 0.012382357133651364, 'n_estimators': 751, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8018457239105959, 'colsample_bytree': 0.904593707655305}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "Warning: XGBoost early stopping not supported. Training without early stopping.\n",
      "Fold 4 RMSE (ensemble): 0.05602 with weights (0.8, 0.2)\n",
      "Fold 4 RMSE (LightGBM only): 0.05602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:15:43,191] A new study created in memory with name: no-name-72f906e8-0252-4daa-94a4-95ca4c18a346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-10 22:15:55,073] Trial 0 finished with value: 0.05671100476279798 and parameters: {'learning_rate': 0.005602247716274752, 'n_estimators': 803, 'num_leaves': 38, 'max_depth': 5, 'min_data_in_leaf': 25, 'subsample': 0.8061867437029867, 'colsample_bytree': 0.756089022489624}. Best is trial 0 with value: 0.05671100476279798.\n",
      "[I 2025-10-10 22:16:03,358] Trial 1 finished with value: 0.05626801012194424 and parameters: {'learning_rate': 0.01768061884336874, 'n_estimators': 513, 'num_leaves': 23, 'max_depth': 6, 'min_data_in_leaf': 11, 'subsample': 0.8756342506068753, 'colsample_bytree': 0.8262596092663939}. Best is trial 1 with value: 0.05626801012194424.\n",
      "[I 2025-10-10 22:16:25,872] Trial 2 finished with value: 0.05607005413391548 and parameters: {'learning_rate': 0.007700982901163797, 'n_estimators': 1636, 'num_leaves': 31, 'max_depth': 10, 'min_data_in_leaf': 12, 'subsample': 0.8583729703615666, 'colsample_bytree': 0.7024619653167091}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:16:35,380] Trial 3 finished with value: 0.05629769445363343 and parameters: {'learning_rate': 0.011852447035835486, 'n_estimators': 748, 'num_leaves': 26, 'max_depth': 5, 'min_data_in_leaf': 26, 'subsample': 0.7302591913086587, 'colsample_bytree': 0.8029435080317036}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:16:47,959] Trial 4 finished with value: 0.056631756410182546 and parameters: {'learning_rate': 0.005014685141114764, 'n_estimators': 759, 'num_leaves': 50, 'max_depth': 6, 'min_data_in_leaf': 18, 'subsample': 0.859454798540596, 'colsample_bytree': 0.8268349477601248}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:17:07,111] Trial 5 finished with value: 0.05610456258327881 and parameters: {'learning_rate': 0.009438912975808483, 'n_estimators': 1338, 'num_leaves': 34, 'max_depth': 7, 'min_data_in_leaf': 30, 'subsample': 0.7275078142592664, 'colsample_bytree': 0.8738293476845217}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:17:18,684] Trial 6 finished with value: 0.05614381791447726 and parameters: {'learning_rate': 0.009535938931983005, 'n_estimators': 699, 'num_leaves': 49, 'max_depth': 6, 'min_data_in_leaf': 26, 'subsample': 0.7442450712172729, 'colsample_bytree': 0.8285379122165771}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:17:30,263] Trial 7 finished with value: 0.05621584643036127 and parameters: {'learning_rate': 0.012632719807228012, 'n_estimators': 828, 'num_leaves': 22, 'max_depth': 9, 'min_data_in_leaf': 33, 'subsample': 0.7200064729282587, 'colsample_bytree': 0.7501480654936048}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:17:50,759] Trial 8 finished with value: 0.05608766253662729 and parameters: {'learning_rate': 0.009726998279133389, 'n_estimators': 1156, 'num_leaves': 35, 'max_depth': 9, 'min_data_in_leaf': 40, 'subsample': 0.7301229144245126, 'colsample_bytree': 0.7907120363054484}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:18:01,137] Trial 9 finished with value: 0.05611795400888376 and parameters: {'learning_rate': 0.017757735998471535, 'n_estimators': 536, 'num_leaves': 35, 'max_depth': 10, 'min_data_in_leaf': 26, 'subsample': 0.8096510908703037, 'colsample_bytree': 0.7145382409885251}. Best is trial 2 with value: 0.05607005413391548.\n",
      "[I 2025-10-10 22:18:23,905] A new study created in memory with name: no-name-2be3b29c-3a30-4c1b-afc4-2beabaff0076\n",
      "[I 2025-10-10 22:18:23,911] Trial 0 finished with value: inf and parameters: {'learning_rate': 0.009524167241658625, 'n_estimators': 551, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.8591303720630518, 'colsample_bytree': 0.8794190008723306}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,914] Trial 1 finished with value: inf and parameters: {'learning_rate': 0.008536140493264299, 'n_estimators': 1474, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.8681669444819238, 'colsample_bytree': 0.8883555052176498}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,917] Trial 2 finished with value: inf and parameters: {'learning_rate': 0.010966975510641316, 'n_estimators': 1019, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8430030496184904, 'colsample_bytree': 0.9164104422205158}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,921] Trial 3 finished with value: inf and parameters: {'learning_rate': 0.010566210092655326, 'n_estimators': 1055, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.8039156140563524, 'colsample_bytree': 0.8133231361620531}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,924] Trial 4 finished with value: inf and parameters: {'learning_rate': 0.008138184237561748, 'n_estimators': 1337, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.9075477965397388, 'colsample_bytree': 0.828075164550652}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,928] Trial 5 finished with value: inf and parameters: {'learning_rate': 0.011280750324995098, 'n_estimators': 1128, 'max_depth': 3, 'min_child_weight': 1, 'subsample': 0.8934506117475896, 'colsample_bytree': 0.8078227850316212}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,932] Trial 6 finished with value: inf and parameters: {'learning_rate': 0.012902110436594906, 'n_estimators': 1492, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.831776869534922, 'colsample_bytree': 0.8308449158903949}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,936] Trial 7 finished with value: inf and parameters: {'learning_rate': 0.005699379695919052, 'n_estimators': 708, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8729163109452389, 'colsample_bytree': 0.9092414683359491}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,940] Trial 8 finished with value: inf and parameters: {'learning_rate': 0.008409777541274801, 'n_estimators': 858, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.9224919682941408, 'colsample_bytree': 0.813587892426848}. Best is trial 0 with value: inf.\n",
      "[I 2025-10-10 22:18:23,945] Trial 9 finished with value: inf and parameters: {'learning_rate': 0.012928482592558606, 'n_estimators': 903, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.8631649337348594, 'colsample_bytree': 0.9407747715093067}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "XGBoost trial failed: XGBModel.fit() got an unexpected keyword argument 'callbacks'\n",
      "Warning: XGBoost early stopping not supported. Training without early stopping.\n",
      "Fold 5 RMSE (ensemble): 0.05598 with weights (0.6, 0.4)\n",
      "Fold 5 RMSE (LightGBM only): 0.05607\n",
      "Loaded v12 predictions for stacking.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 3 features, but LinearRegression is expecting 2 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 396\u001b[39m\n\u001b[32m    393\u001b[39m stacked_test_preds = meta_model.predict(np.column_stack((lgbm_only_test_preds, \n\u001b[32m    394\u001b[39m                                                         np.expm1(xgb_preds / kf.n_splits) \u001b[38;5;28;01mif\u001b[39;00m use_log_transform \u001b[38;5;28;01melse\u001b[39;00m xgb_preds / kf.n_splits)))\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m v12_test_preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     stacked_test_preds = \u001b[43mmeta_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlgbm_only_test_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m                                                            \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_preds\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_log_transform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxgb_preds\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m                                                            \u001b[49m\u001b[43mv12_test_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m stacked_test_preds = np.clip(stacked_test_preds, \u001b[32m0.01\u001b[39m, \u001b[32m0.99\u001b[39m)\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# Additional prediction validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:298\u001b[39m, in \u001b[36mLinearModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    285\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m    Predict using the linear model.\u001b[39;00m\n\u001b[32m    287\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m \u001b[33;03m        Returns predicted values.\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:277\u001b[39m, in \u001b[36mLinearModel._decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    275\u001b[39m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     coef_ = \u001b[38;5;28mself\u001b[39m.coef_\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coef_.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 3 features, but LinearRegression is expecting 2 features as input."
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import hashlib\n",
    "import sys\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Print environment and library versions for debugging\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "\n",
    "# Robust XGBoost version check\n",
    "def get_xgboost_major_version():\n",
    "    try:\n",
    "        version = xgb.__version__\n",
    "        match = re.match(r\"(\\d+)\\.\", version)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "xgb_major_version = get_xgboost_major_version()\n",
    "print(f\"Parsed XGBoost major version: {xgb_major_version}\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print current timestamp\n",
    "print(f\"Script started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Ensure train.csv, test.csv, and sample_submission.csv are in the working directory.\")\n",
    "    exit()\n",
    "\n",
    "print(f'Train shape: {train.shape}')\n",
    "print(f'Test shape: {test.shape}')\n",
    "print(f'Sample shape: {sample.shape}')\n",
    "print(f'Train columns: {train.columns.tolist()}')\n",
    "print('Column data types:')\n",
    "print(train.dtypes)\n",
    "print('Missing values in train:')\n",
    "print(train.isnull().sum())\n",
    "print('Missing values in test:')\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Check target distribution\n",
    "y = train['accident_risk']\n",
    "print(f'Target skewness: {y.skew():.3f}')\n",
    "print(f'Target min: {y.min():.3f}, max: {y.max():.3f}, mean: {y.mean():.3f}')\n",
    "plt.hist(y, bins=30)\n",
    "plt.title('Distribution of accident_risk')\n",
    "plt.show()\n",
    "\n",
    "# Check for zero or negative targets\n",
    "zero_risk = (y == 0).sum()\n",
    "negative_risk = (y < 0).sum()\n",
    "print(f'Zero risk rows: {zero_risk}')\n",
    "print(f'Negative risk rows: {negative_risk}')\n",
    "\n",
    "# Validate columns\n",
    "required_cols = ['id', 'accident_risk', 'speed_limit', 'curvature', 'road_type', \n",
    "                 'lighting', 'weather', 'time_of_day', 'road_signs_present', \n",
    "                 'public_road', 'holiday', 'school_season']\n",
    "missing_cols = [col for col in required_cols if col not in train.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Error: Missing columns in train: {missing_cols}\")\n",
    "    exit()\n",
    "\n",
    "# Preprocessing\n",
    "train_id = train['id']\n",
    "test_id = test['id']\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "# Target (log-transform if skewed)\n",
    "use_log_transform = y.skew() > 1 or y.skew() < -1\n",
    "if use_log_transform:\n",
    "    print('Applying log1p transformation to target due to skewness.')\n",
    "    y = np.log1p(y)\n",
    "\n",
    "train = train.drop('accident_risk', axis=1)\n",
    "\n",
    "# Combine for preprocessing\n",
    "all_data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "# Feature engineering\n",
    "all_data['speed_curv_interact'] = all_data['speed_limit'] * all_data['curvature']\n",
    "all_data['bad_weather'] = np.where(all_data['weather'].isin(['rainy', 'foggy']), 1, 0)\n",
    "all_data['poor_lighting'] = np.where(all_data['lighting'].isin(['dim', 'night']), 1, 0)\n",
    "all_data['rush_hour'] = np.where(all_data['time_of_day'].isin(['morning', 'evening']), 1, 0)\n",
    "all_data['road_condition'] = all_data['curvature'] * all_data['bad_weather']\n",
    "all_data['speed_weather_interact'] = all_data['speed_limit'] * all_data['bad_weather']\n",
    "all_data['curvature_lighting'] = all_data['curvature'] * all_data['poor_lighting']\n",
    "\n",
    "# Encode categoricals\n",
    "cat_cols = ['road_type', 'lighting', 'weather', 'time_of_day']\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    all_data[col] = all_data[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    all_data[col] = le.fit_transform(all_data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Boolean cols to int\n",
    "bool_cols = ['road_signs_present', 'public_road', 'holiday', 'school_season']\n",
    "for col in bool_cols:\n",
    "    all_data[col] = all_data[col].astype(int)\n",
    "\n",
    "# Handle outliers\n",
    "num_cols = ['speed_limit', 'curvature', 'speed_curv_interact', 'road_condition', \n",
    "            'speed_weather_interact', 'curvature_lighting']\n",
    "for col in num_cols:\n",
    "    upper_limit = all_data[col].quantile(0.99)\n",
    "    all_data[col] = np.clip(all_data[col], None, upper_limit)\n",
    "\n",
    "# Data integrity checks\n",
    "numeric_cols = all_data.select_dtypes(include=[np.number]).columns\n",
    "if all_data[numeric_cols].isna().any().any():\n",
    "    print(\"Warning: NaNs found in numeric columns. Filling with median.\")\n",
    "    all_data[numeric_cols] = all_data[numeric_cols].fillna(all_data[numeric_cols].median())\n",
    "if np.isinf(all_data[numeric_cols].values).any():\n",
    "    print(\"Warning: Infinities found in numeric columns. Replacing with max/min values.\")\n",
    "    all_data[numeric_cols] = all_data[numeric_cols].replace([np.inf, -np.inf], \n",
    "                                                            [all_data[numeric_cols].max().max(), \n",
    "                                                             all_data[numeric_cols].min().min()])\n",
    "\n",
    "# Split back\n",
    "train = all_data[:len(train)].reset_index(drop=True)\n",
    "test = all_data[len(train):].reset_index(drop=True)\n",
    "\n",
    "# Prepare X\n",
    "X = train\n",
    "X_test = test\n",
    "\n",
    "print('Preprocessing complete.')\n",
    "\n",
    "# Optuna objective for LightGBM\n",
    "def lgbm_objective(trial, X_train, y_train, X_val, y_val, use_log_transform):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.02, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 40),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9)\n",
    "    }\n",
    "    try:\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='rmse',\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "        )\n",
    "        y_pred = model.predict(X_val)\n",
    "        if use_log_transform:\n",
    "            y_pred = np.expm1(y_pred)\n",
    "            y_val = np.expm1(y_val)\n",
    "        return np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    except Exception as e:\n",
    "        print(f\"LightGBM trial failed: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "# Optuna objective for XGBoost\n",
    "def xgb_objective(trial, X_train, y_train, X_val, y_val, use_log_transform):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.02, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 1500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n",
    "        'subsample': trial.suggest_float('subsample', 0.8, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.8, 0.95)\n",
    "    }\n",
    "    try:\n",
    "        model = xgb.XGBRegressor(**params)\n",
    "        if xgb_major_version >= 2:\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[xgb.callback.EarlyStopping(rounds=100, save_best=True)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            try:\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose=False\n",
    "                )\n",
    "            except TypeError:\n",
    "                print(\"Warning: XGBoost early stopping not supported. Training without early stopping.\")\n",
    "                model.fit(X_train, y_train, verbose=False)\n",
    "        y_pred = model.predict(X_val)\n",
    "        if use_log_transform:\n",
    "            y_pred = np.expm1(y_pred)\n",
    "            y_val = np.expm1(y_val)\n",
    "        return np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "    except Exception as e:\n",
    "        print(f\"XGBoost trial failed: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lgbm_preds = np.zeros(len(X_test))\n",
    "xgb_preds = np.zeros(len(X_test))\n",
    "lgbm_only_preds = np.zeros(len(X_test))\n",
    "oof_lgbm_preds = np.zeros(len(X))\n",
    "oof_xgb_preds = np.zeros(len(X))\n",
    "val_rmse = []\n",
    "val_rmse_lgbm = []\n",
    "\n",
    "# Ensemble weight tuning\n",
    "weight_combinations = [\n",
    "    (0.7, 0.3), (0.8, 0.2), (0.6, 0.4), (0.9, 0.1), (0.75, 0.25), (0.85, 0.15)\n",
    "]\n",
    "\n",
    "best_weights = None\n",
    "best_val_rmse = float('inf')\n",
    "\n",
    "# Cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    print(f'Training fold {fold + 1}...')\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # LightGBM with Optuna\n",
    "    lgbm_study = optuna.create_study(direction='minimize')\n",
    "    lgbm_study.optimize(lambda trial: lgbm_objective(trial, X_train, y_train, X_val, y_val, use_log_transform), n_trials=10)\n",
    "    lgbm_params = lgbm_study.best_params\n",
    "    lgbm_params.update({\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42\n",
    "    })\n",
    "    lgbm_model = lgb.LGBMRegressor(**lgbm_params)\n",
    "    lgbm_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # XGBoost with Optuna\n",
    "    xgb_study = optuna.create_study(direction='minimize')\n",
    "    xgb_study.optimize(lambda trial: xgb_objective(trial, X_train, y_train, X_val, y_val, use_log_transform), n_trials=10)\n",
    "    if not xgb_study.trials:\n",
    "        print(\"Warning: No XGBoost trials completed. Using default parameters.\")\n",
    "        xgb_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42,\n",
    "            'learning_rate': 0.01,\n",
    "            'n_estimators': 1000,\n",
    "            'max_depth': 6,\n",
    "            'min_child_weight': 1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8\n",
    "        }\n",
    "    else:\n",
    "        xgb_params = xgb_study.best_params\n",
    "    xgb_params.update({\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42\n",
    "    })\n",
    "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    try:\n",
    "        if xgb_major_version >= 2:\n",
    "            xgb_model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[xgb.callback.EarlyStopping(rounds=100, save_best=True)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            xgb_model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                early_stopping_rounds=100,\n",
    "                verbose=False\n",
    "            )\n",
    "    except TypeError:\n",
    "        print(\"Warning: XGBoost early stopping not supported. Training without early stopping.\")\n",
    "        xgb_model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    # Validation predictions\n",
    "    lgbm_val_pred = lgbm_model.predict(X_val)\n",
    "    xgb_val_pred = xgb_model.predict(X_val)\n",
    "    \n",
    "    # Save OOF predictions\n",
    "    oof_lgbm_preds[val_idx] = lgbm_val_pred\n",
    "    oof_xgb_preds[val_idx] = xgb_val_pred\n",
    "    \n",
    "    # Tune ensemble weights\n",
    "    fold_best_rmse = float('inf')\n",
    "    fold_best_weights = None\n",
    "    for w_lgbm, w_xgb in weight_combinations:\n",
    "        val_pred = w_lgbm * lgbm_val_pred + w_xgb * xgb_val_pred\n",
    "        if use_log_transform:\n",
    "            val_pred = np.expm1(val_pred)\n",
    "            y_val_temp = np.expm1(y_val)\n",
    "        else:\n",
    "            y_val_temp = y_val\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_temp, val_pred))\n",
    "        if rmse < fold_best_rmse:\n",
    "            fold_best_rmse = rmse\n",
    "            fold_best_weights = (w_lgbm, w_xgb)\n",
    "    \n",
    "    val_rmse.append(fold_best_rmse)\n",
    "    print(f'Fold {fold + 1} RMSE (ensemble): {fold_best_rmse:.5f} with weights {fold_best_weights}')\n",
    "    \n",
    "    # LightGBM-only validation RMSE\n",
    "    lgbm_val_rmse = np.sqrt(mean_squared_error(np.expm1(y_val) if use_log_transform else y_val, \n",
    "                                               np.expm1(lgbm_val_pred) if use_log_transform else lgbm_val_pred))\n",
    "    val_rmse_lgbm.append(lgbm_val_rmse)\n",
    "    print(f'Fold {fold + 1} RMSE (LightGBM only): {lgbm_val_rmse:.5f}')\n",
    "    \n",
    "    # Test predictions\n",
    "    lgbm_preds += lgbm_model.predict(X_test) / kf.n_splits\n",
    "    xgb_preds += xgb_model.predict(X_test) / kf.n_splits\n",
    "    lgbm_only_preds += lgbm_model.predict(X_test) / kf.n_splits\n",
    "    \n",
    "    # Update global best weights\n",
    "    if fold_best_rmse < best_val_rmse:\n",
    "        best_val_rmse = fold_best_rmse\n",
    "        best_weights = fold_best_weights\n",
    "\n",
    "# Stack test predictions\n",
    "test_preds = best_weights[0] * lgbm_preds + best_weights[1] * xgb_preds\n",
    "if use_log_transform:\n",
    "    test_preds = np.expm1(test_preds)\n",
    "lgbm_only_test_preds = np.expm1(lgbm_only_preds) if use_log_transform else lgbm_only_preds\n",
    "\n",
    "# Ensure predictions are between 0 and 1\n",
    "test_preds = np.clip(test_preds, 0.01, 0.99)\n",
    "lgbm_only_test_preds = np.clip(lgbm_only_test_preds, 0.01, 0.99)\n",
    "\n",
    "# Stacking with v12 predictions (if available)\n",
    "try:\n",
    "    v12_submission = pd.read_csv('submission_lgbm_v12.csv')\n",
    "    v12_test_preds = v12_submission['accident_risk'].values\n",
    "    print(\"Loaded v12 predictions for stacking.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: submission_lgbm_v12.csv not found. Using v21 LightGBM predictions as proxy.\")\n",
    "    v12_test_preds = lgbm_only_test_preds\n",
    "\n",
    "# Stack OOF predictions\n",
    "meta_X = np.column_stack((np.expm1(oof_lgbm_preds) if use_log_transform else oof_lgbm_preds, \n",
    "                          np.expm1(oof_xgb_preds) if use_log_transform else oof_xgb_preds))\n",
    "meta_y = np.expm1(y) if use_log_transform else y\n",
    "\n",
    "# Train meta-model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "# Stack test predictions\n",
    "stacked_test_preds = meta_model.predict(np.column_stack((lgbm_only_test_preds, \n",
    "                                                        np.expm1(xgb_preds / kf.n_splits) if use_log_transform else xgb_preds / kf.n_splits)))\n",
    "if v12_test_preds is not None:\n",
    "    stacked_test_preds = meta_model.predict(np.column_stack((lgbm_only_test_preds, \n",
    "                                                            np.expm1(xgb_preds / kf.n_splits) if use_log_transform else xgb_preds / kf.n_splits,\n",
    "                                                            v12_test_preds)))\n",
    "stacked_test_preds = np.clip(stacked_test_preds, 0.01, 0.99)\n",
    "\n",
    "# Additional prediction validation\n",
    "print(f'Ensemble test predictions min: {test_preds.min():.3f}, max: {test_preds.max():.3f}, mean: {test_preds.mean():.3f}')\n",
    "print(f'LightGBM-only test predictions min: {lgbm_only_test_preds.min():.3f}, max: {lgbm_only_test_preds.max():.3f}, mean: {lgbm_only_test_preds.mean():.3f}')\n",
    "print(f'Stacked test predictions min: {stacked_test_preds.min():.3f}, max: {stacked_test_preds.max():.3f}, mean: {stacked_test_preds.mean():.3f}')\n",
    "\n",
    "print(f'Mean CV RMSE (ensemble): {np.mean(val_rmse):.5f}  {np.std(val_rmse):.5f}')\n",
    "print(f'Mean CV RMSE (LightGBM only): {np.mean(val_rmse_lgbm):.5f}  {np.std(val_rmse_lgbm):.5f}')\n",
    "print(f'Best ensemble weights: {best_weights}')\n",
    "\n",
    "# Feature importance\n",
    "lgb.plot_importance(lgbm_model, max_num_features=10)\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Feature selection\n",
    "importances = pd.Series(lgbm_model.feature_importances_, index=X.columns)\n",
    "low_importance = importances[importances < importances.mean() * 0.5].index.tolist()\n",
    "print(f'Low-importance features: {low_importance}')\n",
    "\n",
    "# Save OOF predictions\n",
    "oof_df = pd.DataFrame({\n",
    "    'id': train_id,\n",
    "    'oof_lgbm_pred': np.expm1(oof_lgbm_preds) if use_log_transform else oof_lgbm_preds,\n",
    "    'oof_xgb_pred': np.expm1(oof_xgb_preds) if use_log_transform else oof_xgb_preds\n",
    "})\n",
    "oof_df.to_csv('oof_preds_lgbm_xgb_v22.csv', index=False)\n",
    "print(f'OOF predictions saved as oof_preds_lgbm_xgb_v22.csv')\n",
    "\n",
    "# Submission (ensemble)\n",
    "submission = pd.DataFrame({'id': test_id, 'accident_risk': test_preds})\n",
    "submission_file = 'submission_lgbm_xgb_v22.csv'\n",
    "submission.to_csv(submission_file, index=False)\n",
    "print(f'Ensemble submission saved as {submission_file} at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('Ensemble submission summary:')\n",
    "print(submission.describe())\n",
    "print('Ensemble submission preview (first 10 rows):')\n",
    "print(submission.head(10))\n",
    "\n",
    "# Submission (LightGBM only)\n",
    "submission_lgbm = pd.DataFrame({'id': test_id, 'accident_risk': lgbm_only_test_preds})\n",
    "submission_file_lgbm = 'submission_lgbm_v22.csv'\n",
    "submission_lgbm.to_csv(submission_file_lgbm, index=False)\n",
    "print(f'LightGBM-only submission saved as {submission_file_lgbm} at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('LightGBM-only submission summary:')\n",
    "print(submission_lgbm.describe())\n",
    "print('LightGBM-only submission preview (first 10 rows):')\n",
    "print(submission_lgbm.head(10))\n",
    "\n",
    "# Submission (stacked)\n",
    "submission_stacked = pd.DataFrame({'id': test_id, 'accident_risk': stacked_test_preds})\n",
    "submission_file_stacked = 'submission_stacked_v22.csv'\n",
    "submission_stacked.to_csv(submission_file_stacked, index=False)\n",
    "print(f'Stacked submission saved as {submission_file_stacked} at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('Stacked submission summary:')\n",
    "print(submission_stacked.describe())\n",
    "print('Stacked submission preview (first 10 rows):')\n",
    "print(submission_stacked.head(10))\n",
    "\n",
    "# Calculate file hashes\n",
    "for file in [submission_file, submission_file_lgbm, submission_file_stacked, 'oof_preds_lgbm_xgb_v22.csv']:\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'rb') as f:\n",
    "            file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "        file_size = os.path.getsize(file) / 1024\n",
    "        print(f'{file} size: {file_size:.2f} KB')\n",
    "        print(f'{file} MD5 hash: {file_hash}')\n",
    "    else:\n",
    "        print(f\"Error: {file} was not created.\")\n",
    "\n",
    "# Validate submission format\n",
    "for submission_df, file in [(submission, submission_file), (submission_lgbm, submission_file_lgbm), (submission_stacked, submission_file_stacked)]:\n",
    "    if not all(submission_df.columns == ['id', 'accident_risk']):\n",
    "        print(f\"Error: {file} must have columns 'id' and 'accident_risk'.\")\n",
    "    if len(submission_df) != len(sample):\n",
    "        print(f\"Error: {file} has {len(submission_df)} rows, expected {len(sample)}.\")\n",
    "    if not submission_df['id'].equals(sample['id']):\n",
    "        print(f\"Error: {file} 'id' column does not match sample_submission.csv.\")\n",
    "\n",
    "# Debug: Save raw test predictions\n",
    "raw_preds = pd.DataFrame({'id': test_id, 'ensemble_pred': test_preds, 'lgbm_pred': lgbm_only_test_preds, 'stacked_pred': stacked_test_preds})\n",
    "raw_preds.to_csv('raw_test_preds_lgbm_xgb_v22.csv', index=False)\n",
    "print(f'Raw test predictions saved as raw_test_preds_lgbm_xgb_v22.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

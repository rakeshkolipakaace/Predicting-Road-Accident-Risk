{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2915893f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m norm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEEDS = [42, 181, 2025]\n",
    "np.random.seed(SEEDS[0])\n",
    "\n",
    "# File paths\n",
    "INPUT_DIR = \"/kaggle/input/playground-series-s5e10\"\n",
    "SYN_DIR = \"/kaggle/input/simulated-roads-accident-data\"\n",
    "TRAIN_P = os.path.join(INPUT_DIR, \"train.csv\")\n",
    "TEST_P = os.path.join(INPUT_DIR, \"test.csv\")\n",
    "SAMP_P = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(TRAIN_P)\n",
    "df_test = pd.read_csv(TEST_P)\n",
    "df_sample = pd.read_csv(SAMP_P)\n",
    "\n",
    "# Load synthetic data\n",
    "syn_paths = [\n",
    "    os.path.join(SYN_DIR, f\"synthetic_road_accidents_{s}k.csv\") for s in [2, 10, 100]\n",
    "    if os.path.exists(os.path.join(SYN_DIR, f\"synthetic_road_accidents_{s}k.csv\"))\n",
    "]\n",
    "df_syn = pd.concat([pd.read_csv(p) for p in syn_paths], axis=0, ignore_index=True) if syn_paths else pd.DataFrame()\n",
    "\n",
    "# Data preparation\n",
    "target_col = \"accident_risk\"\n",
    "if target_col not in df_test.columns:\n",
    "    df_test[target_col] = 0.5\n",
    "\n",
    "n_train = len(df_train)\n",
    "n_test = len(df_test)\n",
    "\n",
    "# Add IDs to synthetic data\n",
    "if not df_syn.empty:\n",
    "    if \"id\" not in df_syn.columns:\n",
    "        start_id = int(df_test[\"id\"].max()) + 1\n",
    "        df_syn.insert(0, \"id\", np.arange(start_id, start_id + len(df_syn)))\n",
    "    for c in df_train.columns:\n",
    "        if c not in df_syn.columns:\n",
    "            df_syn[c] = np.nan\n",
    "    df_syn = df_syn[df_train.columns]\n",
    "\n",
    "# Combine datasets\n",
    "df_all = pd.concat([df_train, df_test, df_syn], axis=0, ignore_index=True)\n",
    "print(\"Combined shape:\", df_all.shape)\n",
    "\n",
    "# Convert boolean to int\n",
    "for c in df_all.select_dtypes(include=\"bool\").columns:\n",
    "    df_all[c] = df_all[c].astype(int)\n",
    "\n",
    "# Convert object to string and strip\n",
    "for c in df_all.select_dtypes(include=\"object\").columns:\n",
    "    df_all[c] = df_all[c].astype(str).str.strip()\n",
    "\n",
    "# Baseline risk function\n",
    "def road_risk(X):\n",
    "    return (\n",
    "        0.3 * X[\"curvature\"] +\n",
    "        0.2 * (X[\"lighting\"] == \"night\").astype(int) +\n",
    "        0.1 * (X[\"weather\"] != \"clear\").astype(int) +\n",
    "        0.2 * (X[\"speed_limit\"] >= 60).astype(int) +\n",
    "        0.1 * (X[\"num_reported_accidents\"] > 2).astype(int)\n",
    "    )\n",
    "\n",
    "# Clipped risk function\n",
    "def clipped(func):\n",
    "    def clip_f(X):\n",
    "        mu = func(X)\n",
    "        sigma = 0.05\n",
    "        a, b = -mu / sigma, (1 - mu) / sigma\n",
    "        Phi_a, Phi_b = norm.cdf(a), norm.cdf(b)\n",
    "        phi_a, phi_b = norm.pdf(a), norm.pdf(b)\n",
    "        return mu * (Phi_b - Phi_a) + sigma * (phi_a - phi_b) + 1 - Phi_b\n",
    "    return clip_f\n",
    "\n",
    "df_all[\"y\"] = clipped(road_risk)(df_all)\n",
    "\n",
    "# Advanced feature engineering\n",
    "print(\"Starting advanced feature engineering...\")\n",
    "\n",
    "# Interaction features\n",
    "df_all['road_weather'] = df_all['road_type'].astype(str) + '_' + df_all['weather'].astype(str)\n",
    "df_all['road_light'] = df_all['road_type'].astype(str) + '_' + df_all['lighting'].astype(str)\n",
    "df_all['weather_light'] = df_all['weather'].astype(str) + '_' + df_all['lighting'].astype(str)\n",
    "df_all['curvature_lighting'] = df_all['curvature'] * df_all['lighting'].map({'daylight': 0, 'dim': 1, 'night': 2})\n",
    "df_all['curvature_weather'] = df_all['curvature'] * df_all['weather'].map({'clear': 0, 'rainy': 1, 'foggy': 2})\n",
    "df_all['speed_weather'] = df_all['speed_limit'] * df_all['weather'].map({'clear': 0, 'rainy': 1, 'foggy': 2})\n",
    "\n",
    "# Polynomial features\n",
    "df_all['curvature_sq'] = df_all['curvature'] ** 2\n",
    "df_all['speed_limit_sq'] = df_all['speed_limit'] ** 2\n",
    "\n",
    "# Aggregated features\n",
    "mean_risk_road_type = df_train.groupby('road_type')[target_col].mean().to_dict()\n",
    "df_all['mean_risk_road_type'] = df_all['road_type'].map(mean_risk_road_type)\n",
    "mean_risk_time = df_train.groupby('time_of_day')[target_col].mean().to_dict()\n",
    "df_all['mean_risk_time'] = df_all['time_of_day'].map(mean_risk_time)\n",
    "\n",
    "# Log transformation\n",
    "df_all['log_curvature'] = np.log1p(df_all['curvature'])\n",
    "\n",
    "new_interaction_features = [\n",
    "    'road_weather', 'road_light', 'weather_light',\n",
    "    'curvature_lighting', 'curvature_weather', 'speed_weather',\n",
    "    'curvature_sq', 'speed_limit_sq', 'mean_risk_road_type',\n",
    "    'mean_risk_time', 'log_curvature'\n",
    "]\n",
    "print(f\"Added {len(new_interaction_features)} new features.\")\n",
    "\n",
    "# Categorize features\n",
    "original_cols = [col for col in df_train.columns if col not in ['id', target_col]]\n",
    "CATS, NUMS = [], []\n",
    "CATS.extend(new_interaction_features[:3])  # road_weather, road_light, weather_light\n",
    "for col in original_cols:\n",
    "    if df_all[col].dtype == \"object\":\n",
    "        CATS.append(col)\n",
    "    else:\n",
    "        NUMS.append(col)\n",
    "CATS = list(dict.fromkeys(CATS))\n",
    "\n",
    "# Factorize categorical columns\n",
    "print(f\"Factorizing {len(CATS)} categorical columns: {CATS}\")\n",
    "for col in CATS:\n",
    "    df_all[col], _ = df_all[col].factorize()\n",
    "\n",
    "# Add \"y\" to features\n",
    "FEATURES = CATS + NUMS + ['y'] + new_interaction_features[3:]\n",
    "\n",
    "# Split back into train, test, synthetic\n",
    "df_train_p = df_all.iloc[:n_train].reset_index(drop=True)\n",
    "df_test_p = df_all.iloc[n_train:n_train + n_test].reset_index(drop=True)\n",
    "df_syn_p = df_all.iloc[n_train + n_test:].reset_index(drop=True) if not df_syn.empty else pd.DataFrame()\n",
    "print(\"Sizes -> train:\", len(df_train_p), \"test:\", len(df_test_p), \"synthetic:\", len(df_syn_p))\n",
    "\n",
    "# Target encoding\n",
    "TE_features = []\n",
    "te_source = pd.concat([df_train_p, df_syn_p], axis=0, ignore_index=True)\n",
    "print(f\"Using {len(FEATURES)} features for Target Encoding...\")\n",
    "for col in FEATURES:\n",
    "    te_map = te_source.groupby(col)[target_col].mean()\n",
    "    te_name = f\"TE_{col}\"\n",
    "    df_train_p[te_name] = df_train_p[col].map(te_map)\n",
    "    df_test_p[te_name] = df_test_p[col].map(te_map)\n",
    "    TE_features.append(te_name)\n",
    "print(\"Target Encoding complete.\")\n",
    "\n",
    "# Final feature set\n",
    "FINAL_FEATURES = FEATURES + TE_features\n",
    "print(\"Final feature count:\", len(FINAL_FEATURES))\n",
    "\n",
    "# Initialize storage for predictions\n",
    "FOLDS = 7\n",
    "all_oof_preds = {model: np.zeros((len(SEEDS), len(df_train_p))) for model in ['xgb', 'lgb', 'cat', 'mlp']}\n",
    "all_test_preds = {model: np.zeros((len(SEEDS), len(df_test_p))) for model in ['xgb', 'lgb', 'cat', 'mlp']}\n",
    "\n",
    "# Model parameters\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"eta\": 0.01,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"nthread\": -1,\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 31,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "    \"loss_function\": \"RMSE\",\n",
    "    \"iterations\": 10000,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bylevel\": 0.6,\n",
    "    \"verbose\": 0,\n",
    "    \"early_stopping_rounds\": 200,\n",
    "}\n",
    "\n",
    "# MLP model\n",
    "def create_mlp(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Training loop\n",
    "for seed_idx, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n--- Training with SEED: {seed} ({seed_idx + 1}/{len(SEEDS)}) ---\")\n",
    "    np.random.seed(seed)\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=seed)\n",
    "\n",
    "    for model_name in ['xgb', 'lgb', 'cat', 'mlp']:\n",
    "        oof_preds_seed = np.zeros(len(df_train_p))\n",
    "        test_preds_seed = np.zeros(len(df_test_p))\n",
    "\n",
    "        print(f\"Training {model_name.upper()} with {FOLDS}-fold CV...\")\n",
    "        for fold, (tr_idx, val_idx) in enumerate(kf.split(df_train_p), 1):\n",
    "            print(f\"  Fold {fold}/{FOLDS}\", end=\" ... \")\n",
    "            X_tr = df_train_p.iloc[tr_idx][FINAL_FEATURES]\n",
    "            X_val = df_train_p.iloc[val_idx][FINAL_FEATURES]\n",
    "            y_tr = df_train_p.iloc[tr_idx][target_col].values - df_train_p.iloc[tr_idx][\"y\"].values\n",
    "            y_val = df_train_p.iloc[val_idx][target_col].values - df_train_p.iloc[val_idx][\"y\"].values\n",
    "\n",
    "            if model_name == 'xgb':\n",
    "                dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "                dval = xgb.DMatrix(X_val, label=y_val)\n",
    "                dtest = xgb.DMatrix(df_test_p[FINAL_FEATURES])\n",
    "                model = xgb.train(\n",
    "                    params={**xgb_params, \"seed\": seed},\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=100000,\n",
    "                    evals=[(dtrain, \"train\"), (dval, \"valid\")],\n",
    "                    early_stopping_rounds=200,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                val_pred = model.predict(dval) + df_train_p.iloc[val_idx][\"y\"].values\n",
    "                test_pred = model.predict(dtest) + df_test_p[\"y\"].values\n",
    "\n",
    "            elif model_name == 'lgb':\n",
    "                dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "                dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "                model = lgb.train(\n",
    "                    params={**lgb_params, \"seed\": seed},\n",
    "                    train_set=dtrain,\n",
    "                    valid_sets=[dtrain, dval],\n",
    "                    num_boost_round=10000,\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "                )\n",
    "                val_pred = model.predict(X_val) + df_train_p.iloc[val_idx][\"y\"].values\n",
    "                test_pred = model.predict(df_test_p[FINAL_FEATURES]) + df_test_p[\"y\"].values\n",
    "\n",
    "            elif model_name == 'cat':\n",
    "                dtrain = cb.Pool(X_tr, y_tr)\n",
    "                dval = cb.Pool(X_val, y_val)\n",
    "                model = cb.CatBoostRegressor(**cat_params, random_seed=seed)\n",
    "                model.fit(dtrain, eval_set=dval, use_best_model=True)\n",
    "                val_pred = model.predict(X_val) + df_train_p.iloc[val_idx][\"y\"].values\n",
    "                test_pred = model.predict(df_test_p[FINAL_FEATURES]) + df_test_p[\"y\"].values\n",
    "\n",
    "            elif model_name == 'mlp':\n",
    "                model = create_mlp(len(FINAL_FEATURES))\n",
    "                model.fit(X_tr, y_tr, epochs=50, batch_size=32, verbose=0)\n",
    "                val_pred = model.predict(X_val, verbose=0).flatten() + df_train_p.iloc[val_idx][\"y\"].values\n",
    "                test_pred = model.predict(df_test_p[FINAL_FEATURES], verbose=0).flatten() + df_test_p[\"y\"].values\n",
    "\n",
    "            oof_preds_seed[val_idx] = np.clip(val_pred, 0, 1)\n",
    "            test_preds_seed += np.clip(test_pred, 0, 1) / FOLDS\n",
    "            print(\"done\")\n",
    "\n",
    "        all_oof_preds[model_name][seed_idx] = oof_preds_seed\n",
    "        all_test_preds[model_name][seed_idx] = test_preds_seed\n",
    "        rmse_oof_seed = np.sqrt(mean_squared_error(df_train_p[target_col], oof_preds_seed))\n",
    "        print(f\"SEED {seed} {model_name.upper()} OOF RMSE: {rmse_oof_seed:.5f}\")\n",
    "\n",
    "# Average predictions across seeds\n",
    "final_oof_preds = {}\n",
    "final_test_preds = {}\n",
    "for model_name in all_oof_preds:\n",
    "    final_oof_preds[model_name] = np.mean(all_oof_preds[model_name], axis=0)\n",
    "    final_test_preds[model_name] = np.mean(all_test_preds[model_name], axis=0)\n",
    "    rmse_oof = np.sqrt(mean_squared_error(df_train_p[target_col], final_oof_preds[model_name]))\n",
    "    print(f\"Final {model_name.upper()} OOF RMSE: {rmse_oof:.5f}\")\n",
    "\n",
    "# Stacking\n",
    "print(\"\\nTraining stacking meta-model...\")\n",
    "meta_X = np.column_stack([final_oof_preds[model] for model in ['xgb', 'lgb', 'cat', 'mlp']])\n",
    "meta_model = Ridge()\n",
    "meta_model.fit(meta_X, df_train_p[target_col])\n",
    "meta_test = np.column_stack([final_test_preds[model] for model in ['xgb', 'lgb', 'cat', 'mlp']])\n",
    "stacked_preds = meta_model.predict(meta_test)\n",
    "\n",
    "# Calibration\n",
    "print(\"Calibrating predictions...\")\n",
    "ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "ir.fit(final_oof_preds['xgb'], df_train_p[target_col])  # Use XGBoost as base for calibration\n",
    "final_test_preds_calibrated = ir.predict(stacked_preds)\n",
    "final_test_preds_calibrated = np.clip(final_test_preds_calibrated, 0, 1)\n",
    "\n",
    "# Save submission\n",
    "df_sample[target_col] = final_test_preds_calibrated\n",
    "df_sample.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nWrote submission.csv (preview):\")\n",
    "print(df_sample.head())\n",
    "\n",
    "# Save OOF predictions\n",
    "df_train_p[\"oof_pred\"] = meta_model.predict(meta_X)\n",
    "df_train_p[[\"id\", \"oof_pred\"]].to_csv(\"oof_predictions.csv\", index=False)\n",
    "\n",
    "# Final RMSE\n",
    "rmse_oof_final = np.sqrt(mean_squared_error(df_train_p[target_col], df_train_p[\"oof_pred\"]))\n",
    "rmse_prior = np.sqrt(mean_squared_error(df_train_p[target_col], df_train_p[\"y\"]))\n",
    "print(f\"\\n--- Final Results ---\")\n",
    "print(f\"Baseline prior RMSE: {rmse_prior:.5f}\")\n",
    "print(f\"Final Stacked OOF RMSE: {rmse_oof_final:.5f}\")\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
